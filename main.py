import os
import random
import argparse
import numpy as np
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import albumentations as A
from dataloader.dataset import MedicalDataSets
from utils.util import AverageMeter
import utils.losses as losses
from utils.metrics import iou_score
import csv
from network.MSAA_Unet import MSAA_Unet
from torch.optim.lr_scheduler import CosineAnnealingLR
import time

import datetime
import cv2
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # å…³é”®æ–°å¢è¡Œ
# é…ç½®å‚æ•°ç±»
class Config:
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.seed = 41

        self.model_name = " MSAA_Unet"

        self.img_size = 256
        self.max_epochs = 300

        # è·¯å¾„é…ç½®
        self.base_dir = "./data/busi"
        # self.base_dir = "./data/BUS"
        self.train_file = "busi_train1337.txt"
        # self.train_file = "BUS_train1337.txt"
        self.val_file = "busi_val1337.txt"
        # self.val_file = "BUS_val1337.txt"
        self.checkpoint_dir = "./result"
        self.log_dir = "./result"

        # è®­ç»ƒå‚æ•°
        self.batch_size = 8
        self.num_workers = 8
        self.learning_rate = 1e-4
        self.weight_decay = 1e-4
        self.min_lr = 1e-5


def seed_everything(seed):
    """è®¾ç½®å…¨å±€éšæœºç§å­"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def get_transforms(config, phase='train'):
    """è·å–æ•°æ®å¢å¼ºç»„åˆ"""
    if phase == 'train':
        return A.Compose([
            A.Resize(config.img_size, config.img_size),  # ç»Ÿä¸€å°ºå¯¸
            # å‡ ä½•å˜æ¢
            A.Rotate(limit=(-20, 20), p=0.5),  # éšæœºæ—‹è½¬[-20Â°, 20Â°]
            A.Flip(p=0.5),  # æ°´å¹³å’Œå‚ç›´ç¿»è½¬
            A.ShiftScaleRotate(
                shift_limit=0.05,  # 5%å¹³ç§»
                scale_limit=0.1,  # 10%ç¼©æ”¾
                rotate_limit=0,  # ç¦ç”¨é¢å¤–æ—‹è½¬
                border_mode=cv2.BORDER_CONSTANT,
                p=0.5
            ),
            # å¯¹æ¯”åº¦å¢å¼ºï¼ˆCLAHEæ›´é€‚åˆåŒ»å­¦å½±åƒï¼‰
            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),#ç”¨äºå¯¹å›¾åƒè¿›è¡Œå¯¹æ¯”åº¦é™åˆ¶è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–
            # äº®åº¦/å¯¹æ¯”åº¦å¾®è°ƒï¼ˆÂ±20%ï¼‰
            A.RandomBrightnessContrast(
                brightness_limit=(-0.2, 0.2),
                contrast_limit=(-0.2, 0.2),
                p=0.3
            ),
            # æ·»åŠ è¶…å£°æ–‘ç‚¹å™ªå£°ï¼ˆé«˜æ–¯+æ³Šæ¾æ··åˆï¼‰
            A.GaussNoise(var_limit=(10, 50), p=0.3),#é«˜æ–¯å™ªéŸ³ï¼ŒæŒ‡å®šäº†æ–¹å·®èŒƒå›´ï¼Œæ–¹å·®è¶Šå¤§ï¼Œå™ªå£°è¶Šå¤šã€‚
            A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.3), p=0.3),#ISOå™ªå£°åœ¨æ¨¡æ‹Ÿç›¸æœºåœ¨é«˜ISOï¼ˆæ„Ÿå…‰åº¦ï¼‰ä¸‹æ‹æ‘„æ—¶çš„å™ªç‚¹æ•ˆæœã€‚ISOå™ªå£°é€šå¸¸åŒ…æ‹¬ä¸¤ç§ç±»å‹çš„å™ªå£°ï¼šè‰²åº¦å™ªå£°å’Œäº®åº¦å™ªå£°ã€‚

            A.ElasticTransform(
                alpha=1,  # å½¢å˜å¼ºåº¦
                sigma=2,  # å¹³æ»‘ç³»æ•°
                alpha_affine=5,  # ä»¿å°„å˜æ¢å¼ºåº¦
                border_mode=cv2.BORDER_CONSTANT,
                p=0.1  # ä½æ¦‚ç‡ä½¿ç”¨
            ),
            A.CoarseDropout(#ï¼ˆå³éšæœºé®æŒ¡å›¾åƒçš„ä¸€éƒ¨åˆ†ï¼‰ã€‚è¿™ç§æŠ€æœ¯å¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ åˆ°å¯¹å›¾åƒä¸­å±€éƒ¨ç¼ºå¤±æˆ–é®æŒ¡çš„é²æ£’æ€§ï¼Œå°¤å…¶é€‚ç”¨äºåŒ»å­¦å›¾åƒå¤„ç†ï¼Œ
                max_holes=2,#è¡¨ç¤ºæ¯ä¸ªå›¾åƒä¸Šæœ€å¤šå¯ä»¥åˆ›å»º2ä¸ªçŸ©å½¢é®æŒ¡åŒºåŸŸã€‚
                max_height=0.2,  # æœ€å¤§é®æŒ¡é«˜åº¦ä¸º20%å›¾åƒ
                max_width=0.2,#æœ€å¤§å®½åº¦å¯ä»¥æ˜¯å›¾åƒå®½åº¦çš„20%ã€‚
                min_holes=1,#è¡¨ç¤ºæ¯ä¸ªå›¾åƒä¸Šè‡³å°‘è¦åˆ›å»º1ä¸ªçŸ©å½¢é®æŒ¡åŒºåŸŸã€‚
                fill_value=0,  # å¡«å……é»‘è‰²ï¼ˆæ¨¡æ‹Ÿè¶…å£°å›¾åƒèƒŒæ™¯ï¼‰
                mask_fill_value=None,#è¡¨ç¤ºé®æŒ¡åŒºåŸŸåœ¨æ©ç å›¾åƒä¸Šä¸è¿›è¡Œç‰¹å®šçš„å¡«å……ï¼Œä¿æŒåŸæœ‰å€¼
                p=0.2
            ),
            A.Normalize()
        ])
    return A.Compose([
        A.Resize(config.img_size, config.img_size),
        A.Normalize()
    ])


def create_dataloaders(config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_ds = MedicalDataSets(
        base_dir=config.base_dir,
        split="train",
        transform=get_transforms(config, 'train'),
        train_file_dir=config.train_file,
        val_file_dir=config.val_file,

    )

    val_ds = MedicalDataSets(
        base_dir=config.base_dir,
        split="val",
        transform=get_transforms(config, 'val'),
        train_file_dir=config.train_file,
        val_file_dir=config.val_file,

    )

    print(f"è®­ç»ƒé›†æ•°é‡: {len(train_ds)}, éªŒè¯é›†æ•°é‡: {len(val_ds)}")

    return (
        DataLoader(
            train_ds,
            batch_size=config.batch_size,
            shuffle=True,
            num_workers=config.num_workers,
            pin_memory=False,

        ),
        DataLoader(
            val_ds,
            batch_size=4,
            shuffle=False,
            num_workers=4,

        )
    )


def initialize_model(config):
    """æ¨¡å‹åˆå§‹åŒ–"""
    model_registry = {
        " MSAA_Unet":  MSAA_Unet,
        # "CMUNet": CMUNet,
        # "AAUnet": AAUnet,
        # "Elsknet": Elsknet
    }

    return model_registry[config.model_name]().cuda()


def train_epoch(model, loader, criterion, optimizer):
    """è®­ç»ƒå•ä¸ªepoch"""
    model.train()
    metrics = {
        'loss': AverageMeter(),
        'iou': AverageMeter(),
        'dice': AverageMeter()
    }

    for batch in loader:
        images = batch['image'].cuda(non_blocking=True)
        masks = batch['label'].cuda(non_blocking=True)

        # å‰å‘ä¼ æ’­
        outputs = model(images)
        loss = criterion(outputs, masks)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # æ›´æ–°æŒ‡æ ‡
        iou, dice, *_ = iou_score(outputs, masks)
        metrics['loss'].update(loss.item(), images.size(0))
        metrics['iou'].update(iou, images.size(0))
        metrics['dice'].update(dice, images.size(0))

    return metrics


def validate(model, loader, criterion):
    """éªŒè¯è¿‡ç¨‹"""
    model.eval()
    metrics = {
        'loss': AverageMeter(),
        'iou': AverageMeter(),
        'dice': AverageMeter(),
        'recall': AverageMeter(),
        'precision': AverageMeter(),
        'f1': AverageMeter(),
        'specificity': AverageMeter(),
        'acc': AverageMeter()
    }

    with torch.no_grad():
        for batch in loader:
            images = batch['image'].cuda(non_blocking=True)
            masks = batch['label'].cuda(non_blocking=True)

            outputs = model(images)
            loss = criterion(outputs, masks)

            iou, dice, recall, precision, f1, specificity, acc = iou_score(outputs, masks)

            metrics['loss'].update(loss.item(), images.size(0))
            metrics['iou'].update(iou, images.size(0))
            metrics['dice'].update(dice, images.size(0))
            metrics['recall'].update(recall, images.size(0))
            metrics['precision'].update(precision, images.size(0))
            metrics['f1'].update(f1, images.size(0))
            metrics['specificity'].update(specificity, images.size(0))
            metrics['acc'].update(acc, images.size(0))

    return metrics


def main(config):
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")

    # åœ¨åˆå§‹åŒ–timestampä¹‹åå®šä¹‰æœ€ä½³æ¨¡å‹è·¯å¾„
    best_model_name = f"{config.model_name}_best_{timestamp}.pth"
    best_model_path = os.path.join(config.checkpoint_dir, best_model_name)

    # åˆå§‹åŒ–ç¯å¢ƒ
    seed_everything(config.seed)
    os.makedirs(config.checkpoint_dir, exist_ok=True)
    os.makedirs(config.log_dir, exist_ok=True)

    # åˆå§‹åŒ–ç»„ä»¶
    train_loader, val_loader = create_dataloaders(config)
    model = initialize_model(config)


    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)


    scheduler = CosineAnnealingLR(optimizer, T_max=config.max_epochs , eta_min=config.min_lr)

    criterion = losses.BCEDiceLoss().cuda()

    # æ—¥å¿—è®°å½•ï¼ˆä¿®æ”¹æ–‡ä»¶åï¼‰
    csv_path = os.path.join(config.log_dir, f'training_log_{config.model_name}_{timestamp}.csv')
    best_iou = 0.0

    # æ–°å¢ä¿å­˜å¤šä¸ªæœ€ä½³ IoU åŠå…¶æŒ‡æ ‡çš„åˆ—è¡¨
    num_best_metrics = 3  # é»˜è®¤ä¿å­˜3ä¸ªæœ€ä½³æŒ‡æ ‡ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
    best_metrics_history = []  # ä¿å­˜å¤šä¸ªæœ€ä½³æŒ‡æ ‡

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config.max_epochs):
        start_time = time.time()

        # è®­ç»ƒé˜¶æ®µ
        train_metrics = train_epoch(model, train_loader, criterion, optimizer)

        # éªŒè¯é˜¶æ®µ
        val_metrics = validate(model, val_loader, criterion)



        scheduler.step()

        # å°†å½“å‰éªŒè¯æŒ‡æ ‡åŠ å…¥åˆ°å†å²è®°å½•ä¸­
        current_iou = val_metrics['iou'].avg
        current_metrics = {k: v.avg for k, v in val_metrics.items()}
        current_metrics['epoch'] = epoch

        # æ›´æ–°æœ€ä½³æŒ‡æ ‡å†å²è®°å½•
        # ä¿ç•™æœ€å¤š `num_best_metrics` ä¸ªæœ€ä½³æŒ‡æ ‡
        best_metrics_history = sorted(
            best_metrics_history + [current_metrics],
            key=lambda x: x['iou'],
            reverse=True
        )[:num_best_metrics]

        # è®°å½•æ—¥å¿—
        log_data = [
            epoch + 1,
            optimizer.param_groups[0]['lr'],
            train_metrics['loss'].avg,
            train_metrics['iou'].avg,
            train_metrics['dice'].avg,
            val_metrics['loss'].avg,
            val_metrics['iou'].avg,
            val_metrics['dice'].avg,
            val_metrics['recall'].avg,
            val_metrics['precision'].avg,
            val_metrics['f1'].avg,
            val_metrics['specificity'].avg,
            val_metrics['acc'].avg,
        ]

        # å†™å…¥CSV
        with open(csv_path, 'a', newline='') as f:
            writer = csv.writer(f)
            if epoch == 0:
                writer.writerow(['Epoch', 'LR', 'Train Loss', 'Train IoU', 'Train Dice',
                                 'Val Loss', 'Val IoU', 'Val Dice', 'Recall', 'Precision',
                                 'F1', 'Specificity', 'Accuracy'])
            writer.writerow(log_data)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if current_iou > best_iou:
            best_iou = current_iou
            torch.save(model.state_dict(), best_model_path)
            print(f"ğŸš€ æ›´æ–°æœ€ä½³æ¨¡å‹: {os.path.abspath(best_model_path)}")

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        epoch_time = time.time() - start_time
        print(f"\nEpoch {epoch + 1}/{config.max_epochs} | è€—æ—¶: {epoch_time:.1f}s")
        print(f"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
        print("è®­ç»ƒæŒ‡æ ‡:")
        print(
            f"  Loss: {train_metrics['loss'].avg:.4f} | IoU: {train_metrics['iou'].avg:.4f} | Dice: {train_metrics['dice'].avg:.4f}")
        print("éªŒè¯æŒ‡æ ‡:")
        print(
            f"  Loss: {val_metrics['loss'].avg:.4f} | IoU: {val_metrics['iou'].avg:.4f} | Dice: {val_metrics['dice'].avg:.4f}")
        print("è¯¦ç»†æŒ‡æ ‡:")
        print(f"  Recall:    {val_metrics['recall'].avg:.4f}")
        print(f"  Precision: {val_metrics['precision'].avg:.4f}")
        print(f"  F1:        {val_metrics['f1'].avg:.4f}")
        print(f"  Specificity: {val_metrics['specificity'].avg:.4f}")
        print(f"  Accuracy:  {val_metrics['acc'].avg:.4f}")
        print("-" * 80)

        print(f"æ›´æ–°æœ€ä½³å†å²è®°å½• (å‰ {num_best_metrics} ä¸ª)ï¼š")
        for metrics in best_metrics_history:
            print(metrics)

    # æœ€ç»ˆè¾“å‡º
    print("\nğŸ”¥ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³æ¨¡å‹è·¯å¾„: {os.path.abspath(best_model_path)}")
    print(f"æœ€ä½³éªŒè¯IoU: {best_iou:.4f}")

    # åˆå§‹åŒ–å˜é‡
    last_three_epochs = []

    # å°è¯•è¯»å–æœ€åä¸‰è½®çš„æ•°æ®
    try:
        with open(csv_path, 'r') as f:
            reader = csv.reader(f)
            rows = list(reader)
            if len(rows) <= 1:  # ç¡®ä¿æœ‰æ•°æ®
                print("CSVæ—¥å¿—æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—æœ€åä¸‰è½®")
            else:
                data_rows = rows[1:]  # è·³è¿‡è¡¨å¤´
                last_three_rows = data_rows[-3:]  # å–æœ€åä¸‰è¡Œ

                for row in last_three_rows:
                    # è§£ææ¯è¡Œæ•°æ®ï¼Œå‡è®¾åˆ—é¡ºåºä¸å†™å…¥æ—¶ä¸€è‡´
                    epoch_num, lr, train_loss, train_iou, train_dice, \
                        val_loss, val_iou, val_dice, recall, precision, \
                        f1, specificity, acc = row
                    last_three_epochs.append({
                        'iou': float(val_iou),
                        'dice': float(val_dice),
                        'recall': float(recall),
                        'precision': float(precision),
                        'f1': float(f1),
                        'specificity': float(specificity),
                        'acc': float(acc)
                    })
    except FileNotFoundError:
        print(f"æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶: {csv_path}")

    # æ‰“å°æœ€åä¸‰è½®çš„æŒ‡æ ‡
    if last_three_epochs:
        print("\næœ€åä¸‰è½®æŒ‡æ ‡ï¼š")
        for metrics in last_three_epochs:
            print(metrics)

    # è®¡ç®—æœ€åä¸‰è½®æŒ‡æ ‡çš„å‡å€¼å’Œæ ‡å‡†å·®
    if last_three_epochs:
        ious = [m['iou'] for m in last_three_epochs]
        dice = [m['dice'] for m in last_three_epochs]
        recall = [m['recall'] for m in last_three_epochs]
        precision = [m['precision'] for m in last_three_epochs]
        f1 = [m['f1'] for m in last_three_epochs]
        specificity = [m['specificity'] for m in last_three_epochs]
        acc = [m['acc'] for m in last_three_epochs]

        iou_mean = np.mean(ious)
        iou_std = np.std(ious)
        dice_mean = np.mean(dice)
        dice_std = np.std(dice)
        recall_mean = np.mean(recall)
        recall_std = np.std(recall)
        precision_mean = np.mean(precision)
        precision_std = np.std(precision)
        f1_mean = np.mean(f1)
        f1_std = np.std(f1)
        specificity_mean = np.mean(specificity)
        specificity_std = np.std(specificity)
        acc_mean = np.mean(acc)
        acc_std = np.std(acc)

        print("\næœ€åä¸‰è½®æŒ‡æ ‡çš„å‡å€¼å’Œæ ‡å‡†å·®:")
        print(f"IoU:        {iou_mean:.8f} Â± {iou_std:.8f}")
        print(f"Dice:       {dice_mean:.8f} Â± {dice_std:.8f}")
        print(f"Recall:     {recall_mean:.8f} Â± {recall_std:.8f}")
        print(f"Precision:  {precision_mean:.8f} Â± {precision_std:.8f}")
        print(f"F1:         {f1_mean:.8f} Â± {f1_std:.8f}")
        print(f"Specificity: {specificity_mean:.8f} Â± {specificity_std:.8f}")
        print(f"Accuracy:   {acc_mean:.8f} Â± {acc_std:.8f}")

        # ä¿å­˜åˆ° CSV æ–‡ä»¶
        summary_path = os.path.join(config.log_dir, f'last_three_metrics_summary_{timestamp}.csv')
        with open(summary_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Metric', 'Mean', 'Std'])
            writer.writerow(['IoU', f"{iou_mean:.8f}", f"{iou_std:.8f}"])
            writer.writerow(['Dice', f"{dice_mean:.8f}", f"{dice_std:.8f}"])
            writer.writerow(['Recall', f"{recall_mean:.8f}", f"{recall_std:.8f}"])
            writer.writerow(['Precision', f"{precision_mean:.8f}", f"{precision_std:.8f}"])
            writer.writerow(['F1', f"{f1_mean:.8f}", f"{f1_std:.8f}"])
            writer.writerow(['Specificity', f"{specificity_mean:.8f}", f"{specificity_std:.8f}"])
            writer.writerow(['Accuracy', f"{acc_mean:.8f}", f"{acc_std:.8f}"])


if __name__ == "__main__":
    config = Config()
    main(config)
